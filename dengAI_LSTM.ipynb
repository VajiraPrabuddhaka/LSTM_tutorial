{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import Utilities.myutil_regr as myutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "\n",
    "dataset = pandas.read_csv('dengue_labels_train.csv', usecols=[3], engine='python', skipfooter=3)\n",
    "\n",
    "dataset1 = pandas.read_csv('dengue_features_train.csv')\n",
    "dataset2 = pandas.read_csv('dengue_labels_train.csv')\n",
    "\n",
    "iq, sj = myutil.split_dataset_by_city(dataset2)\n",
    "iq = iq.drop(columns=['city', 'year', 'weekofyear'])\n",
    "sj = sj.drop(columns=['city', 'year', 'weekofyear'])\n",
    "\n",
    "dfall_iq, dfall_sj = myutil.split_dataset_by_city(dataset1)\n",
    "\n",
    "dfall_iq = dfall_iq.drop(columns=['city', 'year', 'weekofyear', 'week_start_date'])\n",
    "dfall_sj = dfall_sj.drop(columns=['city', 'year', 'weekofyear', 'week_start_date'])\n",
    "\n",
    "\n",
    "dfall_iq['total_cases'] = Series(iq['total_cases'], index=dfall_iq.index)\n",
    "dfall_sj['total_cases'] = Series(sj['total_cases'], index=dfall_iq.index)\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "dfall_iq.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n   0.   1.   1.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n   0.   0.   0.   0.   1.   1.   0.   0.   1.   0.   0.   0.   0.   0.\n   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n   0.   0.   1.   1.   1.   2.   4.   1.   4.  11.  16.  23.  12.  14.\n  18.   8.   7.  10.   7.  10.   5.  11.   8.  18.  13.   9.  22.  10.\n   5.  13.   2.  11.  11.   3.   7.   7.   4.   5.   6.   7.   7.   4.\n   9.  17.   8.  22.  18.  21.  16.  31.  25.  28.  26.  18.  27.  11.\n  38.  29.  21.  11.  10.   5.   6.   2.   1.   2.   2.   3.   5.   1.\n   4.   2.   4.   0.   0.   0.   0.   1.   1.   1.   1.   1.   2.   3.\n   4.   6.   2.   2.   5.   1.   1.   0.   0.   0.   0.   2.   0.   3.\n   0.   0.   0.   2.   2.   3.   3.   3.   1.   2.   3.   6.   5.   1.\n   4.   5.   8.   5.   2.   3.   3.   1.   6.   4.   1.   2.   3.   1.\n   8.   4.   6.   7.   5.   8.   6.   5.   6.   6.  13.   2.  10.   3.\n  12.   7.   6.   5.   6.   6.   6.   8.   6.   9.  12.  19.   8.  16.\n  21.   6.  22.  37.  33.  18.  83. 116.  32.   7.   9.  10.   5.   8.\n   7.   8.  11.   6.   7.   7.  14.   7.   9.  13.  16.   7.   9.   2.\n  13.   8.   3.   5.   4.   8.   2.   3.   5.   7.   3.   5.   6.   5.\n   5.   4.   0.   0.   0.   0.   0.   2.   4.   4.   3.   3.   5.   6.\n  14.   3.   7.  11.   2.   6.   8.  25.  21.  10.  28.  39.  20.  24.\n  28.  26.   8.   9.  12.  18.   9.   9.   6.   6.   8.   5.   7.   6.\n   5.   3.   1.   0.   1.   2.   3.   2.   2.   2.   2.   2.   4.   0.\n   6.   3.   2.   6.   2.   7.   4.   6.   6.   2.  13.  10.   5.   2.\n   0.   1.   0.  14.   6.  10.   5.  12.   9.   5.  11.   2.   6.   7.\n   6.   5.   9.   5.   8.   3.   4.  11.   5.   8.   4.   3.   1.   2.\n   3.   4.   1.   8.   5.   3.   2.   7.   1.   6.   7.   5.   2.   6.\n  11.   6.   3.  11.  11.   5.   4.   9.  23.  28.  26.   7.  29.  58.\n  26.  38.  35.  37.  20.  29.  25.  23.   9.   3.   6.   6.   3.   1.\n   3.   1.   1.   0.   2.   1.   1.   0.   0.   1.   0.   3.   3.   1.\n   5.   2.   5.   5.   5.   9.  17.  19.  25.  45.  34.  63.  44.  50.\n  35.  16.  16.  13.   9.  15.   4.   0.   1.  10.  11.  29.  35.  30.\n  20.  21.  12.   9.  11.   9.   5.  11.   3.   5.   5.   4.   4.   1.\n   0.   2.   3.   3.   5.   2.   1.   2.   0.   0.   3.   5.   5.   7.\n   5.   2.   2.   2.   0.   2.   1.   1.   2.   2.   3.   9.   5.   5.\n   4.   4.   1.   0.   0.  10.   9.  17.  16.  11.  12.  19.  15.  12.\n  12.  16.   9.   4.   9.   6.   8.   4.   2.   7.   6.   5.   8.   1.\n   1.   4.]\n<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# convert series to supervised learning\n",
    "from pandas import DataFrame, concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "values = dfall_iq.values\n",
    "# integer encode direction\n",
    "print(values[:, 20])\n",
    "encoder = LabelEncoder()\n",
    "values[:,20] = encoder.fit_transform(values[:,20])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "#print (reframed)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]], axis=1, inplace=True)\n",
    "#reframed.drop([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20], axis=1, inplace=True)\n",
    "print(type(reframed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "train = values\n",
    "print (train.shape)\n",
    "\n",
    "train_X, train_y = train[:, :-1], train[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "\tdataX, dataY = [], []\n",
    "\tfor i in range(len(dataset)-look_back-1):\n",
    "\t\ta = dataset[i:(i+look_back), 0]\n",
    "\t\tdataX.append(a)\n",
    "\t\tdataY.append(dataset[i + look_back, 0])\n",
    "\treturn np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_back = 1\n",
    "\n",
    "trainX, trainY = create_dataset(dataset, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cf2d83cf8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(1, look_back)))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
